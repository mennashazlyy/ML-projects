{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv file of data by pandas ana sorting by industry\n",
    "data = pd.read_csv(\"Job titles and industries.csv\")\n",
    "data.sort_values(\"industry\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing duplicate job title\n",
    "data = data.drop_duplicates(subset =\"job title\",keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning from punctuation \n",
    "data['cleaned'] = data['job title'].apply(lambda x:''.join([i for i in x if i not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing job titles by nltk\n",
    "data['tokenized_sents'] = data.apply(lambda row: nltk.word_tokenize(row['cleaned']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embbding using glove \n",
    "#reading glove data\n",
    "\n",
    "def load_file_glove():\n",
    "    dict = {}\n",
    "    with open('glove.6B.300d.txt', 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            dict[line.split()[0]] = list(map(float,line.split()[1:]))\n",
    "        f.close()\n",
    "    return  dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = load_file_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to search in glove dictionary and in data then get its vector and if not found will make vector of zeros dim=300\n",
    "\n",
    "def search_dict(data, dict):\n",
    "    total_list = []\n",
    "    for list in data:\n",
    "        res_list = []\n",
    "        for word in list:\n",
    "            if word in dict.keys():\n",
    "                res_list.append(dict[word])\n",
    "            else:\n",
    "                res_list.append([0]*300)\n",
    "        total_list.append(res_list)\n",
    "    return total_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_glove_data = search_dict(data['tokenized_sents'],glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this method sums all vectors of words in sent. to give one vector for each sentence (job title)\n",
    "#sentence embedding \n",
    "\n",
    "def sum_method(data):\n",
    "    sum_list = []\n",
    "    temp_sum_list = []\n",
    "    for x in data:\n",
    "        temp_sum_list = sum(np.array(x))\n",
    "        sum_list.append(temp_sum_list.tolist())\n",
    "    return sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sum = sum_method(all_glove_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will put label for each class\n",
    "# ACCOUNTANCY 0\n",
    "#EDUCATION 1\n",
    "#IT 2\n",
    "#MARKETING 3\n",
    "\n",
    "def get_label(size_data,label):\n",
    "    label_list =[]\n",
    "    for i in range(size_data):\n",
    "        label_list.append(label)\n",
    "    return label_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(data.loc[data['industry'] == 'Accountancy']) #263\n",
    "#len(data.loc[data['industry'] == 'Education']) #972\n",
    "#len(data.loc[data['industry'] == 'IT']) #1514\n",
    "#len(data.loc[data['industry'] == 'Marketing']) #1141\n",
    "\n",
    "acc_label = get_label(263,0)\n",
    "ed_label = get_label(972,1)\n",
    "it_label = get_label(1514,2)\n",
    "mark_label = get_label(1141,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function concat to lists to put all labels in one list\n",
    "\n",
    "def concat_data(list1,list2):\n",
    "    list_all = []\n",
    "    for l1 in list1 :\n",
    "        list_all.append(l1)\n",
    "    for l2 in list2:\n",
    "         list_all.append(l2)\n",
    "    return list_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = concat_data(acc_label,ed_label)\n",
    "list2 = concat_data(list1,it_label)\n",
    "\n",
    "all_label = concat_data(list2,mark_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dealing with imbalanced data by over sampling it\n",
    "\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(all_sum, all_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split train and test\n",
    "\n",
    "def Train_Test_Split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Train_Test_Split(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i used log_reg for multiclass and linear-svm model it gives accuracy 0.905 but they fail to converage and its better to use KNN and KNN very easy to implement for multi-class problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classifier_log(X_train, X_test, y_train, y_test):\n",
    "#     LogReg = LogisticRegression(class_weight='balanced',random_state=0,solver='sag',C=1e7, multi_class='multinomial',max_iter=10000)\n",
    "#     LogReg.fit(X_train, y_train)\n",
    "#     y_pred = LogReg.predict(X_test)\n",
    "#     c_matrix = confusion_matrix(y_test, y_pred)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     return LogReg,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalar = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scalar = scaler.fit_transform(X_train)  # compute mean, std and transform training data as well\n",
    "# X_test_scalar = scaler.transform(X_test)\n",
    "# def classification_LSVM(X_train, X_test, y_train, y_test):\n",
    "#     svm  = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,intercept_scaling=1, loss='squared_hinge', max_iter=1200,multi_class='ovr', penalty='l2' ,random_state=0, tol=1e-05,verbose=0)\n",
    "#     svm.fit(X_train, y_train)\n",
    "#     y_pred = svm.predict(X_test)\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     return svm,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to improve model normalizing data on the same scale as KNN is not suitable for the large dimensional data\n",
    "#evaluation using accuracy\n",
    "#limitations KNN doesn't perform will on imbalanced data and important to features to have the same scale\n",
    "\n",
    "def classify_KNN(X_train, X_test, y_train, y_test):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "    neigh.fit(X_train, y_train)\n",
    "    y_pred = neigh.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    return neigh,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn,acc_knn = classify_KNN(X_train, X_test, y_train, y_test)\n",
    "#acc = 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_log,acc_log = classifier_log(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_model,acc_svm = classification_LSVM(X_train_scalar, X_test_scalar, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_log(log_reg, test , dict ):\n",
    "#     test_list= []\n",
    "#     tokens =nltk.word_tokenize(test)\n",
    "#     test_list.append(tokens)\n",
    "#     x = search_dict(test_list,dict)\n",
    "#     sum_list  =  sum_method(x)\n",
    "#     y_pred = log_reg.predict(sum_list)\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_svm(svm, test , dict ):\n",
    "#     test_list= []\n",
    "#     tokens =nltk.word_tokenize(test)\n",
    "#     test_list.append(tokens)\n",
    "#     x = search_dict(test_list,dict)\n",
    "#     sum_list  =  sum_method(x)\n",
    "#     y_pred = svm.predict(sum_list)\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_KNN(KNN_model, test , dict ):\n",
    "    test_list= []\n",
    "    tokens =nltk.word_tokenize(test)\n",
    "    test_list.append(tokens)\n",
    "    x = search_dict(test_list,dict)\n",
    "    sum_list  =  sum_method(x)\n",
    "    y_pred = KNN_model.predict(sum_list)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(prediction):\n",
    "    if prediction ==0 :\n",
    "        print('Accountancy')\n",
    "    elif prediction == 1:\n",
    "        print('Education')\n",
    "    elif prediction == 2:\n",
    "        print('IT')\n",
    "    else :\n",
    "        print('Marketing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter test= trainee accountant\n",
      "Accountancy\n"
     ]
    }
   ],
   "source": [
    "input_test = input('enter test= ')\n",
    "prediction= predict_KNN(model_knn,input_test,glove_dict)\n",
    "output(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
